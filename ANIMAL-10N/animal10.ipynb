{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guzhii/CAS_771_Project/blob/main/ANIMAL-10N/animal10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc5pdxwumzSj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Global variables\n",
        "lr = 0.002\n",
        "result_dir= 'results/'\n",
        "noise_rate = 0.08\n",
        "forget_rate = 0.08\n",
        "num_gradual = 20\n",
        "exponent = 1\n",
        "top_bn = False\n",
        "dataset = 'animal10'\n",
        "NUM_CLASSES = 10\n",
        "n_epoch=120\n",
        "seed=1\n",
        "print_freq=50\n",
        "num_workers=4\n",
        "num_iter_per_epoch=500\n",
        "epoch_decay_start=80\n",
        "\n",
        "# Seed\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Hyper Parameters\n",
        "batch_size = 100\n",
        "learning_rate = lr \n",
        "RANDOM_SEED = 1\n",
        "batch_size = 100\n",
        "epochs = 120\n",
        "\n",
        "# Architecture\n",
        "NUM_FEATURES = 64*64\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Other\n",
        "device = \"cuda\"\n",
        "GRAYSCALE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKrHBHBq2art"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "############ Copy raw image dataset of ANIMAL-10N from google drive ############\n",
        "################################################################################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir /content/imageData\n",
        "mkdir /content/imageData/testing\n",
        "mkdir /content/imageData/training"
      ],
      "metadata": {
        "id": "vEwxUk6cSRT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4HWyZdP2d5g"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cp -R /content/drive/MyDrive/LiamXu/testing /content/imageData/testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -R /content/drive/MyDrive/LiamXu/training/0/ /content/imageData/training/0/"
      ],
      "metadata": {
        "id": "OPgwKkTxTEzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -R /content/drive/MyDrive/LiamXu/training/0/ /content/imageData/training/1/"
      ],
      "metadata": {
        "id": "1mAJr4ptTKyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -R /content/drive/MyDrive/LiamXu/training/0/ /content/imageData/training/2/"
      ],
      "metadata": {
        "id": "IUuFG7F1TKpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -R /content/drive/MyDrive/LiamXu/training/0/ /content/imageData/training/3/"
      ],
      "metadata": {
        "id": "5kNW1TNtTKiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -R /content/drive/MyDrive/LiamXu/training/0/ /content/imageData/training/4/"
      ],
      "metadata": {
        "id": "ug6cr6w6TKa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -R /content/drive/MyDrive/LiamXu/training/0/ /content/imageData/training/5/"
      ],
      "metadata": {
        "id": "_dYSLjkJTKT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -R /content/drive/MyDrive/LiamXu/training/0/ /content/imageData/training/6/"
      ],
      "metadata": {
        "id": "JZcohJjTTKMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -R /content/drive/MyDrive/LiamXu/training/0/ /content/imageData/training/7/"
      ],
      "metadata": {
        "id": "hI1-uRffTKDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -R /content/drive/MyDrive/LiamXu/training/0/ /content/imageData/training/8/"
      ],
      "metadata": {
        "id": "DfnA3kLKTJ6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -R /content/drive/MyDrive/LiamXu/training/0/ /content/imageData/training/9/"
      ],
      "metadata": {
        "id": "xhQe3KK8TJxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weNo5FdalGt6"
      },
      "outputs": [],
      "source": [
        "from os import listdir, mkdir,rename\n",
        "from os.path import isfile, join\n",
        "\n",
        "onlyfiles = [f for f in listdir(\"/content/imageData/testing/\") if isfile(join(\"/content/imageData/testing/\", f))]\n",
        "\n",
        "for i in range(10):\n",
        "  mkdir(\"/content/imageData/testing/\" + str(i))\n",
        "\n",
        "for f in onlyfiles:\n",
        "    if f[0] == \"0\":\n",
        "        rename(\"/content/imageData/testing/\"+f, \"/content/imageData/testing/0/\"+f)\n",
        "    elif f[0] == \"1\":\n",
        "        rename(\"/content/imageData/testing/\"+f, \"/content/imageData/testing/1/\"+f)\n",
        "    elif f[0] == \"2\":\n",
        "        rename(\"/content/imageData/testing/\"+f, \"/content/imageData/testing/2/\"+f)\n",
        "    elif f[0] == \"3\":\n",
        "        rename(\"/content/imageData/testing/\"+f, \"/content/imageData/testing/3/\"+f)\n",
        "    elif f[0] == \"4\":\n",
        "        rename(\"/content/imageData/testing/\"+f, \"/content/imageData/testing/4/\"+f)\n",
        "    elif f[0] == \"5\":\n",
        "        rename(\"/content/imageData/testing/\"+f, \"/content/imageData/testing/5/\"+f)\n",
        "    elif f[0] == \"6\":\n",
        "        rename(\"/content/imageData/testing/\"+f, \"/content/imageData/testing/6/\"+f)\n",
        "    elif f[0] == \"7\":\n",
        "        rename(\"/content/imageData/testing/\"+f, \"/content/imageData/testing/7/\"+f)\n",
        "    elif f[0] == \"8\":\n",
        "        rename(\"/content/imageData/testing/\"+f, \"/content/imageData/testing/8/\"+f)\n",
        "    elif f[0] == \"9\":\n",
        "        rename(\"/content/imageData/testing/\"+f, \"/content/imageData/testing/9/\"+f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j7OaPwtieuf"
      },
      "outputs": [],
      "source": [
        "class CustomFolder(datasets.ImageFolder):\n",
        "    def __getitem__(self, index):        \n",
        "        return super(datasets.ImageFolder, self).__getitem__(index), index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx1rLVqQm4-K"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "test_set = datasets.ImageFolder('imageData/testing/', transform=transform)\n",
        "train_set = CustomFolder('imageData/training/', transform=transform)\n",
        "#train_set, val_set = torch.utils.data.random_split(dataset, [int(len(dataset)*.8), len(dataset)-int(len(dataset)*.8)])\n",
        "noisy_labels = np.array(train_set.targets)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100,\n",
        "                                           shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8cz4ydDUjht"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "############################## ResNet-34 model #################################\n",
        "################################################################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = conv3x3(3, 64) # number 1 indicate how many channels\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(2048 * block.expansion, num_classes)\n",
        "        self.c_linear = nn.Linear(2048 * block.expansion,1)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, lin=0, lout=5):\n",
        "        out = x\n",
        "        if lin < 1 and lout > -1:\n",
        "            out = self.conv1(out)\n",
        "            out = self.bn1(out)\n",
        "            out = F.relu(out)\n",
        "        if lin < 2 and lout > 0:\n",
        "            out = self.layer1(out)\n",
        "        if lin < 3 and lout > 1:\n",
        "            out = self.layer2(out)\n",
        "        if lin < 4 and lout > 2:\n",
        "            out = self.layer3(out)\n",
        "        if lin < 5 and lout > 3:\n",
        "            out = self.layer4(out)\n",
        "        if lout > 4:\n",
        "            out = F.avg_pool2d(out, 4)\n",
        "            out = out.view(out.size(0), -1)\n",
        "            feature = out\n",
        "            out_c = self.c_linear(out)\n",
        "            out = self.linear(out)\n",
        "        return out, out_c\n",
        "\n",
        "def resnet34(num_classes=10):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=NUM_CLASSES)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlFonVGyUnZl"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "############ Setup for optimizer and forget-rate schedule ######################\n",
        "################################################################################\n",
        "\n",
        "import os\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "import argparse, sys\n",
        "import numpy as np\n",
        "import datetime\n",
        "import shutil\n",
        "\n",
        "# Adjust learning rate and betas for Adam Optimizer\n",
        "mom1 = 0.9\n",
        "mom2 = 0.1\n",
        "alpha_plan = [learning_rate] * n_epoch\n",
        "beta1_plan = [mom1] * n_epoch\n",
        "for i in range(epoch_decay_start, n_epoch):\n",
        "    alpha_plan[i] = float(n_epoch - i) / (n_epoch - epoch_decay_start) * learning_rate\n",
        "    beta1_plan[i] = mom2\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr']=alpha_plan[epoch]\n",
        "        param_group['betas']=(beta1_plan[epoch], 0.999) # Only change beta1\n",
        "        \n",
        "# define drop rate schedule\n",
        "rate_schedule = np.ones(n_epoch)*forget_rate\n",
        "rate_schedule[:num_gradual] = np.linspace(0, forget_rate**exponent, num_gradual)\n",
        "   \n",
        "save_dir = result_dir +'/' +dataset+'/coteaching/'\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.system('mkdir -p %s' % save_dir)\n",
        "\n",
        "model_str=dataset+'_coteaching_'+'_'+str(noise_rate)\n",
        "\n",
        "txtfile=save_dir+\"/\"+model_str+str(num_gradual)+\".txt\"\n",
        "nowTime=datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "if os.path.exists(txtfile):\n",
        "    os.system('mv %s %s' % (txtfile, txtfile+\".bak-%s\" % nowTime))\n",
        "\n",
        "\n",
        "def accuracy(logit, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    output = F.softmax(logit, dim=1)\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwD6HvWrU9p8"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "######################## Confidence adaptive loss ##############################\n",
        "################################################################################\n",
        "epi = 1e-12\n",
        "ep_threshold = 60\n",
        "momentum = 0.9\n",
        "lossLamda = 50\n",
        "regularization_strength = 0\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "\n",
        "class CAR(torch.nn.Module):\n",
        "    def __init__(self, labels, num_classes):\n",
        "        super(CAR, self).__init__()\n",
        "        self.num_classes = NUM_CLASSES\n",
        "        self.threshold_update = 0.0\n",
        "\n",
        "        self.soft_labels = torch.zeros(labels.shape[0], num_classes, dtype=torch.float).cuda(non_blocking=True)\n",
        "        self.soft_labels[torch.arange(labels.shape[0]), labels] = 1\n",
        "        self.momentum = momentum\n",
        "        self.beta = regularization_strength\n",
        "        self.es = ep_threshold\n",
        "        self.test = 0\n",
        "        if torch.cuda.is_available():\n",
        "          torch.backends.cudnn.benchmark = True\n",
        "          if torch.cuda.device_count() > 1:\n",
        "              self.device = torch.device('cuda:0')\n",
        "          else:\n",
        "              self.device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "\n",
        "    def forward(self, logits, confidence,  labels, index, lam, epoch):\n",
        "        # sigmoid scale 0 to 1\n",
        "        confidence = torch.sigmoid(confidence)\n",
        "\n",
        "        output = F.softmax(logits, dim=1)\n",
        "        eps = 1e-12\n",
        "        output = torch.clamp(output, 0. + eps, 1. - eps)\n",
        "        confidence = torch.clamp(confidence, 0. + eps, 1. - eps)\n",
        "        one_hot = torch.zeros(len(labels), self.num_classes)\n",
        "        one_hot[torch.arange(len(labels)), labels] = 1\n",
        "        one_hot = one_hot.to(device)\n",
        "        one_hot = torch.clamp(one_hot, min=1e-4, max=1.0)  # A=-4\n",
        "        labels = labels.to(self.device)\n",
        "        confidence = confidence.to(self.device)\n",
        "        output = output.to(self.device)\n",
        "\n",
        "        if epoch < ep_threshold:\n",
        "            pred = confidence * output + (1 - confidence) * one_hot\n",
        "            pred = torch.clamp(pred, min=1e-7, max=1.0)\n",
        "            loss1 = -torch.mean(torch.sum(torch.log(pred) * one_hot, dim=1))\n",
        "            rce = (-1 * torch.sum(pred * torch.log(one_hot), dim=1))\n",
        "        else:\n",
        "            if epoch % 10 == 0:\n",
        "                temp_p = F.softmax(logits.detach(), dim=1)\n",
        "                temp_p = temp_p.to(self.device)\n",
        "                tp_f = confidence > self.threshold_update # only change the data has confidence >= threshold\n",
        "                change_index = index[tp_f.view(tp_f.size()[0])]\n",
        "                tp_f = tp_f.repeat(1, self.num_classes)\n",
        "                self.soft_labels[change_index] = self.momentum * self.soft_labels[change_index] + (\n",
        "                        1 - self.momentum) * temp_p[tp_f].view(-1, self.num_classes)\n",
        "                self.soft_labels = torch.clamp(self.soft_labels, min=1e-4, max=1.0)\n",
        "            pred = confidence * output + (1 - confidence) * self.soft_labels[index]\n",
        "            pred = torch.clamp(pred, min=1e-7, max=1.0)\n",
        "            loss1 = -torch.mean(torch.sum(torch.log(pred) * self.soft_labels[index], dim=1))\n",
        "            rce = (-1 * torch.sum(pred * torch.log(self.soft_labels[index]), dim=1))\n",
        "\n",
        "        loss2 = -torch.mean(torch.log(confidence))\n",
        "        return loss1 + lam * loss2 + self.beta * rce.mean()\n",
        "criterion = CAR(noisy_labels, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXAr8PMYWvfg"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "################## image mix-up function and co-teaching loss ##################\n",
        "################################################################################\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "\n",
        "# Loss functions\n",
        "def loss_coteaching(y_1, y_2, c_1, c_2, t_a, t_b, forget_rate, ind, lam, ep_num):\n",
        "    # loss_1_a = criterion(y_1, c_1, t_a, ind, ep_num)\n",
        "    # loss_1_b = criterion(y_1, c_1, t_b, ind, ep_num)\n",
        "    loss_1_a = F.cross_entropy(y_1, t_a, reduce = False)\n",
        "    loss_1_b = F.cross_entropy(y_1, t_b, reduce = False)\n",
        "    loss_1 = torch.add(torch.mul(loss_1_a, lam), torch.mul(loss_1_b, (1-lam)))\n",
        "    ind_1_sorted = np.argsort(loss_1.cpu().data).cuda()\n",
        "    loss_1_sorted = loss_1[ind_1_sorted]\n",
        "\n",
        "    # loss_2_a = criterion(y_2, c_2, t_a, ind, ep_num)\n",
        "    # loss_2_b = criterion(y_2, c_2, t_b, ind, ep_num)\n",
        "    loss_2_a = F.cross_entropy(y_2, t_a, reduce = False)\n",
        "    loss_2_b = F.cross_entropy(y_2, t_b, reduce = False)\n",
        "    loss_2 = torch.add(torch.mul(loss_2_a, lam), torch.mul(loss_2_b, (1-lam)))\n",
        "    ind_2_sorted = np.argsort(loss_2.cpu().data).cuda()\n",
        "    loss_2_sorted = loss_2[ind_2_sorted]\n",
        "\n",
        "    remember_rate = 1 - forget_rate\n",
        "    num_remember = int(remember_rate * len(loss_1_sorted))\n",
        "\n",
        "    #pure_ratio_1 = np.sum(noise_or_not[ind[ind_1_sorted.cpu()[:num_remember]]])/float(num_remember)\n",
        "    #pure_ratio_2 = np.sum(noise_or_not[ind[ind_2_sorted.cpu()[:num_remember]]])/float(num_remember)\n",
        "\n",
        "    ind_1_update=ind_1_sorted[:num_remember]\n",
        "    ind_2_update=ind_2_sorted[:num_remember]\n",
        "\n",
        "    # exchange\n",
        "    loss_1_a_update = criterion(y_1[ind_2_update], c_1[ind_2_update], t_a[ind_2_update], ind[ind_2_update], 50, ep_num)\n",
        "    loss_1_b_update = criterion(y_1[ind_2_update], c_1[ind_2_update], t_b[ind_2_update], ind[ind_2_update], 50, ep_num)\n",
        "    # loss_1_a_update = F.cross_entropy(y_1[ind_2_update], t_a[ind_2_update])\n",
        "    # loss_1_b_update = F.cross_entropy(y_1[ind_2_update], t_b[ind_2_update])\n",
        "    loss_1_update = torch.add(torch.mul(loss_1_a_update, lam), torch.mul(loss_1_b_update, (1-lam)))\n",
        "    loss_2_a_update = criterion(y_2[ind_1_update], c_2[ind_1_update], t_a[ind_1_update], ind[ind_1_update], 50, ep_num)\n",
        "    loss_2_b_update = criterion(y_2[ind_1_update], c_2[ind_1_update], t_b[ind_1_update], ind[ind_1_update], 50, ep_num)\n",
        "    # loss_2_a_update = F.cross_entropy(y_2[ind_1_update], t_a[ind_1_update])\n",
        "    # loss_2_b_update = F.cross_entropy(y_2[ind_1_update], t_b[ind_1_update])\n",
        "    loss_2_update = torch.add(torch.mul(loss_2_a_update, lam), torch.mul(loss_2_b_update, (1-lam)))\n",
        "\n",
        "    return loss_1_update, loss_2_update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74uv8xBGWyIr"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "######################### Model training and testing ###########################\n",
        "################################################################################\n",
        "\n",
        "## Train the Model\n",
        "def train(train_loader,epoch, model1, optimizer1, model2, optimizer2):\n",
        "    print('Training %s...' % model_str)\n",
        "    \n",
        "    train_total=0\n",
        "    train_correct=0 \n",
        "    train_total2=0\n",
        "    train_correct2=0 \n",
        "\n",
        "\n",
        "    for i, ((images, labels), index) in enumerate(train_loader):\n",
        "        #ind=indexes.cpu().numpy().transpose()\n",
        "        #if i>num_iter_per_epoch:\n",
        "        #    break\n",
        "      \n",
        "        images = Variable(images).cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "        #mixup\n",
        "\n",
        "        images, labels_a, labels_b, lam = mixup_data(images, labels,\n",
        "                                                       1, 1)\n",
        "        images, labels_a, labels_b = map(Variable, (images,\n",
        "                                                      labels_a, labels_b))\n",
        "        \n",
        "        # Forward + Backward + Optimize\n",
        "        outputs1, confidence1 =model1(images)\n",
        "        train_total+=outputs1.size(0)\n",
        "        _, predicted = torch.max(outputs1.data, 1)\n",
        "        train_correct+=(lam * predicted.eq(labels_a.data).cpu().sum().float()\n",
        "                    + (1 - lam) * predicted.eq(labels_b.data).cpu().sum().float())\n",
        "\n",
        "        outputs2, confidence2 = model2(images)\n",
        "        train_total2+=outputs2.size(0)\n",
        "        _, predicted = torch.max(outputs2.data, 1)\n",
        "        train_correct2+=(lam * predicted.eq(labels_a.data).cpu().sum().float()\n",
        "                    + (1 - lam) * predicted.eq(labels_b.data).cpu().sum().float())\n",
        "        \n",
        "        loss_1, loss_2 = loss_coteaching(outputs1.cpu(), outputs2.cpu(), confidence1.cpu(), confidence2.cpu(), labels.cpu(), labels_b.cpu(), rate_schedule[epoch], index, lam, epoch)\n",
        "\n",
        "        optimizer1.zero_grad()\n",
        "        loss_1.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.zero_grad()\n",
        "        loss_2.backward()\n",
        "        optimizer2.step()\n",
        "        if (i+1) % print_freq == 0:\n",
        "            print ('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f' \n",
        "                  %(epoch+1, n_epoch, i+1, len(train_set)//batch_size, 100.*train_correct/train_total, 100.*train_correct2/train_total2, loss_1.data, loss_2.data))\n",
        "\n",
        "    train_acc1=float(train_correct)/float(train_total)\n",
        "    train_acc2=float(train_correct2)/float(train_total2)\n",
        "    return train_acc1, train_acc2\n",
        "\n",
        "# Evaluate the Model\n",
        "def evaluate(test_loader, model1, model2):\n",
        "    print('Evaluating %s...' % model_str)\n",
        "    model1.eval()    # Change model to 'eval' mode.\n",
        "    correct1 = 0\n",
        "    total1 = 0\n",
        "    accTable1 = [ [0]*10 for _ in range(10) ]\n",
        "    for images, labels in test_loader:\n",
        "        images = Variable(images).cuda()\n",
        "        logits1, _ = model1(images)\n",
        "        outputs1 = F.softmax(logits1, dim=1)\n",
        "        _, pred1 = torch.max(outputs1.data, 1)\n",
        "        total1 += labels.size(0)\n",
        "        correct1 += (pred1.cpu() == labels).sum()\n",
        "        predTmp1 = pred1.cpu().tolist()\n",
        "        labelsTmp = labels.tolist()\n",
        "        for i in range(len(predTmp1)):\n",
        "            accTable1[predTmp1[i]][labelsTmp[i]] += 1\n",
        "\n",
        "    model2.eval()    # Change model to 'eval' mode \n",
        "    correct2 = 0\n",
        "    total2 = 0\n",
        "    accTable2 = [ [0]*10 for _ in range(10) ]\n",
        "    for images, labels in test_loader:\n",
        "        images = Variable(images).cuda()\n",
        "        logits2, _ = model2(images)\n",
        "        outputs2 = F.softmax(logits2, dim=1)\n",
        "        _, pred2 = torch.max(outputs2.data, 1)\n",
        "        total2 += labels.size(0)\n",
        "        correct2 += (pred2.cpu() == labels).sum()\n",
        "        predTmp2 = pred2.cpu().tolist()\n",
        "        labelsTmp = labels.tolist()\n",
        "        for i in range(len(predTmp1)):\n",
        "            accTable2[predTmp2[i]][labelsTmp[i]] = accTable2[predTmp2[i]][labelsTmp[i]] + 1\n",
        " \n",
        "    acc1 = 100*float(correct1)/float(total1)\n",
        "    acc2 = 100*float(correct2)/float(total2)\n",
        "    with open(txtfile, \"a\") as myfile:\n",
        "        myfile.write(str(accTable1))\n",
        "        myfile.write(str(accTable1))\n",
        "    return acc1, acc2\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('building model...')\n",
        "\n",
        "    cnn1 = resnet34().to(device)\n",
        "    optimizer1 = torch.optim.Adam(cnn1.parameters(), lr=learning_rate)\n",
        "\n",
        "    cnn2 = resnet34().to(device)\n",
        "    optimizer2 = torch.optim.Adam(cnn2.parameters(), lr=learning_rate)\n",
        "\n",
        "    mean_pure_ratio1=0\n",
        "    mean_pure_ratio2=0\n",
        "\n",
        "    with open(txtfile, \"a\") as myfile:\n",
        "        myfile.write('epoch: train_acc1 train_acc2 test_acc1 test_acc2 pure_ratio1 pure_ratio2\\n')\n",
        "\n",
        "    epoch=0\n",
        "    train_acc1=0\n",
        "    train_acc2=0\n",
        "    # evaluate models with random weights\n",
        "    test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
        "    print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %% Pure Ratio1 %.4f %% Pure Ratio2 %.4f %%' % (epoch+1, n_epoch, len(test_set), test_acc1, test_acc2, mean_pure_ratio1, mean_pure_ratio2))\n",
        "    # save results\n",
        "    with open(txtfile, \"a\") as myfile:\n",
        "        myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + ' '  + str(mean_pure_ratio1) + ' '  + str(mean_pure_ratio2) + \"\\n\")\n",
        "\n",
        "    # training\n",
        "    for epoch in range(1, n_epoch):\n",
        "        # train models\n",
        "        cnn1.train()\n",
        "        adjust_learning_rate(optimizer1, epoch)\n",
        "        cnn2.train()\n",
        "        adjust_learning_rate(optimizer2, epoch)\n",
        "        train_acc1, train_acc2=train(train_loader, epoch, cnn1, optimizer1, cnn2, optimizer2)\n",
        "        # evaluate models\n",
        "        test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
        "        # save results\n",
        "        print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%' % (epoch+1, n_epoch, len(test_set), test_acc1, test_acc2))\n",
        "        with open(txtfile, \"a\") as myfile:\n",
        "            myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + \"\\n\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "animal10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMPFGkpVRFA+u0gaOuvnUK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}