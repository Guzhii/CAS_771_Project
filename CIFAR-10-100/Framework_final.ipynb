{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guzhii/CAS_771_Project/blob/main/CIFAR-10-100/Framework_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VXjbvP-yF1BU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Global variables\n",
        "device = \"cuda\"\n",
        "GRAYSCALE = False\n",
        "lr = 0.002\n",
        "result_dir= 'results/'\n",
        "noise_rate = 0\n",
        "forget_rate = None\n",
        "num_gradual = 40\n",
        "exponent = 1\n",
        "top_bn = False\n",
        "dataset = 'cifar10'\n",
        "NUM_CLASSES = 10\n",
        "n_epoch=120\n",
        "seed=1\n",
        "print_freq=50\n",
        "num_workers=4\n",
        "num_iter_per_epoch=500\n",
        "epoch_decay_start=80\n",
        "\n",
        "# Seed\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Hyper Parameters\n",
        "batch_size = 100\n",
        "learning_rate = lr "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZheJS7M1FuyF"
      },
      "outputs": [],
      "source": [
        "####################################################################\n",
        "############### Noisy Loader  ######################################\n",
        "####################################################################\n",
        "\n",
        "def unpickle(file):\n",
        "    import _pickle as cPickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = cPickle.load(fo, encoding='latin1')\n",
        "    return dict\n",
        "\n",
        "\n",
        "class cifar_dataset(Dataset):\n",
        "    def __init__(self, dataset, root_dir, transform, mode, noise_file=''):\n",
        "\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.transition = {0: 0, 2: 0, 4: 7, 7: 7, 1: 1, 9: 1, 3: 5, 5: 3, 6: 6,\n",
        "                           8: 8}  # class transition for asymmetric noise for cifar10\n",
        "        # generate asymmetric noise for cifar100\n",
        "        self.transition_cifar100 = {}\n",
        "        nb_superclasses = 20\n",
        "        nb_subclasses = 5\n",
        "        base = [1, 2, 3, 4, 0]\n",
        "        for i in range(nb_superclasses * nb_subclasses):\n",
        "            self.transition_cifar100[i] = int(base[i % 5] + 5 * int(i / 5))\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            if dataset == 'cifar10':\n",
        "                test_dic = unpickle('%s/test_batch' % root_dir)\n",
        "                self.test_data = test_dic['data']\n",
        "                self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
        "                self.test_data = self.test_data.transpose((0, 2, 3, 1))\n",
        "                self.test_label = test_dic['labels']\n",
        "            elif dataset == 'cifar100':\n",
        "                test_dic = unpickle('%s/test' % root_dir)\n",
        "                self.test_data = test_dic['data']\n",
        "                self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
        "                self.test_data = self.test_data.transpose((0, 2, 3, 1))\n",
        "                self.test_label = test_dic['fine_labels']\n",
        "        else:\n",
        "            train_data = []\n",
        "            train_label = []\n",
        "            if dataset == 'cifar10':\n",
        "                for n in range(1, 6):\n",
        "                    dpath = '%s/data_batch_%d' % (root_dir, n)\n",
        "                    data_dic = unpickle(dpath)\n",
        "                    train_data.append(data_dic['data'])\n",
        "                    train_label = train_label + data_dic['labels']\n",
        "                train_data = np.concatenate(train_data)\n",
        "            elif dataset == 'cifar100':\n",
        "                train_dic = unpickle('%s/train' % root_dir)\n",
        "                train_data = train_dic['data']\n",
        "                train_label = train_dic['fine_labels']\n",
        "                # print(train_label)\n",
        "                # print(len(train_label))\n",
        "            train_data = train_data.reshape((50000, 3, 32, 32))\n",
        "            train_data = train_data.transpose((0, 2, 3, 1))\n",
        "\n",
        "            noise_label = json.load(open(noise_file, \"r\"))\n",
        "\n",
        "            if self.mode == 'train':\n",
        "                self.train_data = train_data\n",
        "                self.noise_label = noise_label\n",
        "                self.clean_label = train_label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.mode == 'train':\n",
        "            img, target = self.train_data[index], self.noise_label[index]\n",
        "            img = Image.fromarray(img)\n",
        "            img = self.transform(img)\n",
        "            return img, target, index\n",
        "        elif self.mode == 'test':\n",
        "            img, target = self.test_data[index], self.test_label[index]\n",
        "            img = Image.fromarray(img)\n",
        "            img = self.transform(img)\n",
        "            return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode != 'test':\n",
        "            return len(self.train_data)\n",
        "        else:\n",
        "            return len(self.test_data)\n",
        "\n",
        "\n",
        "class cifar_dataloader():\n",
        "    def __init__(self, dataset, batch_size, num_workers, root_dir, noise_file=''):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.root_dir = root_dir\n",
        "        self.noise_file = noise_file\n",
        "        if self.dataset == 'cifar10':\n",
        "            self.transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "            ])\n",
        "            self.transform_test = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "            ])\n",
        "        elif self.dataset == 'cifar100':\n",
        "            self.transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n",
        "            ])\n",
        "            self.transform_test = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n",
        "            ])\n",
        "\n",
        "    def run(self, mode):\n",
        "        if mode == 'train':\n",
        "            train_dataset = cifar_dataset(dataset=self.dataset,\n",
        "                                          root_dir=self.root_dir, transform=self.transform_train, mode=\"train\",\n",
        "                                          noise_file=self.noise_file)\n",
        "            trainloader = DataLoader(\n",
        "                dataset=train_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=self.num_workers)\n",
        "            return trainloader, np.asarray(train_dataset.noise_label), np.asarray(train_dataset.clean_label)\n",
        "\n",
        "        elif mode == 'test':\n",
        "            test_dataset = cifar_dataset(dataset=self.dataset,\n",
        "                                         root_dir=self.root_dir, transform=self.transform_test, mode='test')\n",
        "            test_loader = DataLoader(\n",
        "                dataset=test_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=self.num_workers)\n",
        "            return test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SoCdHcZQcEAH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = conv3x3(3, 64) # number 1 indicate how many channels\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "        self.c_linear = nn.Linear(512 * block.expansion,1)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, lin=0, lout=5):\n",
        "        out = x\n",
        "        if lin < 1 and lout > -1:\n",
        "            out = self.conv1(out)\n",
        "            out = self.bn1(out)\n",
        "            out = F.relu(out)\n",
        "        if lin < 2 and lout > 0:\n",
        "            out = self.layer1(out)\n",
        "        if lin < 3 and lout > 1:\n",
        "            out = self.layer2(out)\n",
        "        if lin < 4 and lout > 2:\n",
        "            out = self.layer3(out)\n",
        "        if lin < 5 and lout > 3:\n",
        "            out = self.layer4(out)\n",
        "        if lout > 4:\n",
        "            out = F.avg_pool2d(out, 4)\n",
        "            out = out.view(out.size(0), -1)\n",
        "            feature = out\n",
        "            out_c = self.c_linear(out)\n",
        "            out = self.linear(out)\n",
        "        return out, out_c\n",
        "\n",
        "def resnet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=NUM_CLASSES)\n",
        "\n",
        "\n",
        "# def test():\n",
        "#     net = ResNet34()\n",
        "#     y1, feature1,c1 = net(Variable(torch.randn(3, 3, 32, 32)))\n",
        "#     y2, feature2,c2= net(Variable(torch.randn(3, 3, 32, 32)))\n",
        "#     print(y1.size())\n",
        "#     print(feature1.size())\n",
        "#     print(c1.size())\n",
        "#\n",
        "# test()\n",
        "\n",
        "\n",
        "\n",
        "model = resnet34().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkWmBgOBaP4t",
        "outputId": "b3930d92-0ed1-4bb5-e904-e661331f9b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "loading dataset...\n"
          ]
        }
      ],
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "import os\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "import argparse, sys\n",
        "import numpy as np\n",
        "import datetime\n",
        "import shutil\n",
        "\n",
        "\n",
        "# load dataset\n",
        "if dataset=='cifar10':\n",
        "    input_channel=3\n",
        "    num_classes=10\n",
        "    top_bn = False\n",
        "    epoch_decay_start = 80\n",
        "    \n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "    data_path = '/content/data/cifar-10-batches-py'  # path to the data file (don't forget to download the feature data and also put the noisy label file under this folder)\n",
        "\n",
        "if dataset=='cifar100':\n",
        "    input_channel=3\n",
        "    num_classes=100\n",
        "    top_bn = False\n",
        "    epoch_decay_start = 100\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "    test_set = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "    data_path = '/content/data/cifar-100-python'  # path to the data file (don't forget to download the feature data and also put the noisy label file under this folder)\n",
        "\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "print('loading dataset...')\n",
        "loader = cifar_dataloader(dataset, batch_size=100,\n",
        "                          num_workers=2,\n",
        "                          root_dir=data_path,\n",
        "                          noise_file='%s/cifar10_noisy_labels_task1.json' % (data_path))\n",
        "\n",
        "train_loader, noisy_labels, clean_labels = loader.run('train')\n",
        "test_loader = loader.run('test')\n",
        "\n",
        "noise_or_not = np.transpose(noisy_labels)==np.transpose(clean_labels)\n",
        "noise_rate = noise_or_not.tolist().count(False) / len(noise_or_not)\n",
        "\n",
        "\n",
        "\n",
        "if forget_rate is None:\n",
        "    forget_rate=noise_rate\n",
        "else:\n",
        "    forget_rate=forget_rate\n",
        "\n",
        "# Adjust learning rate and betas for Adam Optimizer\n",
        "mom1 = 0.9\n",
        "mom2 = 0.1\n",
        "alpha_plan = [learning_rate] * n_epoch\n",
        "beta1_plan = [mom1] * n_epoch\n",
        "for i in range(epoch_decay_start, n_epoch):\n",
        "    alpha_plan[i] = float(n_epoch - i) / (n_epoch - epoch_decay_start) * learning_rate\n",
        "    beta1_plan[i] = mom2\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr']=alpha_plan[epoch]\n",
        "        param_group['betas']=(beta1_plan[epoch], 0.999) # Only change beta1\n",
        "        \n",
        "# define drop rate schedule\n",
        "rate_schedule = np.ones(n_epoch)*forget_rate\n",
        "rate_schedule[:num_gradual] = np.linspace(0, forget_rate**exponent, num_gradual)\n",
        "\n",
        "save_dir = result_dir +'/' +dataset+'/framework/'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.system('mkdir -p %s' % save_dir)\n",
        "\n",
        "model_str=dataset+'_framework_'+'_'+str(noise_rate)\n",
        "\n",
        "txtfile=save_dir+\"/\"+model_str+str(num_gradual)+\".txt\"\n",
        "nowTime=datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "if os.path.exists(txtfile):\n",
        "    os.system('mv %s %s' % (txtfile, txtfile+\".bak-%s\" % nowTime))\n",
        "\n",
        "\n",
        "def accuracy(logit, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    output = F.softmax(logit, dim=1)\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7uhpD1r8cqUZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "epi = 1e-12\n",
        "ep_threshold = 60\n",
        "momentum = 0.9\n",
        "lossLamda = 0.5\n",
        "regularization_strength = 0.1\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "\n",
        "class CAR(torch.nn.Module):\n",
        "    def __init__(self, labels, num_classes):\n",
        "        super(CAR, self).__init__()\n",
        "        self.num_classes = NUM_CLASSES\n",
        "        self.threshold_update = 0.0\n",
        "\n",
        "        self.soft_labels = torch.zeros(labels.shape[0], num_classes, dtype=torch.float).cuda(non_blocking=True)\n",
        "        self.soft_labels[torch.arange(labels.shape[0]), labels] = 1\n",
        "        self.momentum = momentum\n",
        "        self.beta = regularization_strength\n",
        "        self.es = ep_threshold\n",
        "        self.test = 0\n",
        "        if torch.cuda.is_available():\n",
        "          torch.backends.cudnn.benchmark = True\n",
        "          if torch.cuda.device_count() > 1:\n",
        "              self.device = torch.device('cuda:0')\n",
        "          else:\n",
        "              self.device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "\n",
        "    def forward(self, logits, confidence,  labels, index, lam, epoch):\n",
        "        # sigmoid scale 0 to 1\n",
        "        confidence = torch.sigmoid(confidence)\n",
        "\n",
        "        output = F.softmax(logits, dim=1)\n",
        "        eps = 1e-12\n",
        "        output = torch.clamp(output, 0. + eps, 1. - eps)\n",
        "        confidence = torch.clamp(confidence, 0. + eps, 1. - eps)\n",
        "        one_hot = torch.zeros(len(labels), self.num_classes)\n",
        "        one_hot[torch.arange(len(labels)), labels] = 1\n",
        "        one_hot = one_hot.to(device)\n",
        "        one_hot = torch.clamp(one_hot, min=1e-4, max=1.0)  # A=-4\n",
        "        labels = labels.to(self.device)\n",
        "        confidence = confidence.to(self.device)\n",
        "        output = output.to(self.device)\n",
        "\n",
        "        if epoch < ep_threshold:\n",
        "            pred = confidence * output + (1 - confidence) * one_hot\n",
        "            pred = torch.clamp(pred, min=1e-7, max=1.0)\n",
        "            loss1 = -torch.mean(torch.sum(torch.log(pred) * one_hot, dim=1))\n",
        "            rce = (-1 * torch.sum(pred * torch.log(one_hot), dim=1))\n",
        "        else:\n",
        "            if epoch % 10 == 0:\n",
        "                temp_p = F.softmax(logits.detach(), dim=1)\n",
        "                temp_p = temp_p.to(self.device)\n",
        "                tp_f = confidence > self.threshold_update # only change the data has confidence >= threshold\n",
        "                change_index = index[tp_f.view(tp_f.size()[0])]\n",
        "                tp_f = tp_f.repeat(1, self.num_classes)\n",
        "                self.soft_labels[change_index] = self.momentum * self.soft_labels[change_index] + (\n",
        "                        1 - self.momentum) * temp_p[tp_f].view(-1, self.num_classes)\n",
        "                self.soft_labels = torch.clamp(self.soft_labels, min=1e-4, max=1.0)\n",
        "            pred = confidence * output + (1 - confidence) * self.soft_labels[index]\n",
        "            pred = torch.clamp(pred, min=1e-7, max=1.0)\n",
        "            loss1 = -torch.mean(torch.sum(torch.log(pred) * self.soft_labels[index], dim=1))\n",
        "            rce = (-1 * torch.sum(pred * torch.log(self.soft_labels[index]), dim=1))\n",
        "\n",
        "        loss2 = -torch.mean(torch.log(confidence))\n",
        "        return loss1 + lam * loss2 + self.beta * rce.mean()\n",
        "criterion = CAR(noisy_labels, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yw8uAGp_BEwd"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "\n",
        "def loss_coteaching(y_1, y_2, c_1, c_2, t_a, t_b, forget_rate, ind, noise_or_not, lam, ep_num):\n",
        "    # Calculate and sort cross entropy losses for both models\n",
        "    loss_1_a = F.cross_entropy(y_1, t_a, reduce = False)\n",
        "    loss_1_b = F.cross_entropy(y_1, t_b, reduce = False)\n",
        "    loss_1 = torch.add(torch.mul(loss_1_a, lam), torch.mul(loss_1_b, (1-lam)))\n",
        "    ind_1_sorted = np.argsort(loss_1.cpu().data).cuda()\n",
        "    loss_1_sorted = loss_1[ind_1_sorted]\n",
        "\n",
        "    loss_2_a = F.cross_entropy(y_2, t_a, reduce = False)\n",
        "    loss_2_b = F.cross_entropy(y_2, t_b, reduce = False)\n",
        "    loss_2 = torch.add(torch.mul(loss_2_a, lam), torch.mul(loss_2_b, (1-lam)))\n",
        "    ind_2_sorted = np.argsort(loss_2.cpu().data).cuda()\n",
        "    loss_2_sorted = loss_2[ind_2_sorted]\n",
        "\n",
        "    # Discard high loss samples based on forget rate\n",
        "    remember_rate = 1 - forget_rate\n",
        "    num_remember = int(remember_rate * len(loss_1_sorted))\n",
        "    pure_ratio_1 = np.sum(noise_or_not[ind[ind_1_sorted.cpu()[:num_remember]]])/float(num_remember)\n",
        "    pure_ratio_2 = np.sum(noise_or_not[ind[ind_2_sorted.cpu()[:num_remember]]])/float(num_remember)\n",
        "    ind_1_update=ind_1_sorted[:num_remember]\n",
        "    ind_2_update=ind_2_sorted[:num_remember]\n",
        "\n",
        "    # exchange and update confidence adaptive losses of both models\n",
        "    loss_1_a_update = criterion(y_1[ind_2_update], c_1[ind_2_update], t_a[ind_2_update], ind[ind_2_update], 50, ep_num)\n",
        "    loss_1_b_update = criterion(y_1[ind_2_update], c_1[ind_2_update], t_b[ind_2_update], ind[ind_2_update], 50, ep_num)\n",
        "    # loss_1_a_update = F.cross_entropy(y_1[ind_2_update], t_a[ind_2_update])\n",
        "    # loss_1_b_update = F.cross_entropy(y_1[ind_2_update], t_b[ind_2_update])\n",
        "    loss_1_update = torch.add(torch.mul(loss_1_a_update, lam), torch.mul(loss_1_b_update, (1-lam)))\n",
        "    loss_2_a_update = criterion(y_2[ind_1_update], c_2[ind_1_update], t_a[ind_1_update], ind[ind_1_update], 50, ep_num)\n",
        "    loss_2_b_update = criterion(y_2[ind_1_update], c_2[ind_1_update], t_b[ind_1_update], ind[ind_1_update], 50, ep_num)\n",
        "    # loss_2_a_update = F.cross_entropy(y_2[ind_1_update], t_a[ind_1_update])\n",
        "    # loss_2_b_update = F.cross_entropy(y_2[ind_1_update], t_b[ind_1_update])\n",
        "    loss_2_update = torch.add(torch.mul(loss_2_a_update, lam), torch.mul(loss_2_b_update, (1-lam)))\n",
        "\n",
        "    return loss_1_update, loss_2_update, pure_ratio_1, pure_ratio_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "Kx2vvuCgCd2X",
        "outputId": "a5672d05-3fe2-41e9-e490-44e36ea8792e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building model...\n",
            "Evaluating cifar10_framework__0.35962...\n",
            "Epoch [1/120] Test Accuracy on the 10000 test images: Model1 9.8100 % Model2 10.0000 % Pure Ratio1 0.0000 % Pure Ratio2 0.0000 %\n",
            "Training cifar10_framework__0.35962...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/120], Iter [50/500] Training Accuracy1: 10.5603, Training Accuracy2: 9.9766, Loss1: 3.3083, Loss2: 3.2801, Pure Ratio1: 64.4242, Pure Ratio2 64.3636\n",
            "Epoch [2/120], Iter [100/500] Training Accuracy1: 11.3664, Training Accuracy2: 10.7590, Loss1: 3.0497, Loss2: 3.1227, Pure Ratio1: 64.3434, Pure Ratio2 64.1515\n",
            "Epoch [2/120], Iter [150/500] Training Accuracy1: 12.2458, Training Accuracy2: 11.3676, Loss1: 3.0966, Loss2: 3.1100, Pure Ratio1: 64.2963, Pure Ratio2 64.1616\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2d7e16739fb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-2d7e16739fb0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mcnn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mtrain_acc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure_ratio_1_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure_ratio_2_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;31m# evaluate models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mtest_acc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-2d7e16739fb0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch, model1, optimizer1, model2, optimizer2)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Get predictions and confidence value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moutputs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtrain_total\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-80605d73d13c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lin, lout)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlin\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-80605d73d13c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## Train the Model\n",
        "def train(train_loader,epoch, model1, optimizer1, model2, optimizer2):\n",
        "    print('Training %s...' % model_str)\n",
        "    pure_ratio_list=[]\n",
        "    pure_ratio_1_list=[]\n",
        "    pure_ratio_2_list=[]\n",
        "    \n",
        "    train_total=0\n",
        "    train_correct=0 \n",
        "    train_total2=0\n",
        "    train_correct2=0 \n",
        "\n",
        "    for i, (images, labels, indexes) in enumerate(train_loader):\n",
        "        if i>num_iter_per_epoch:\n",
        "            break\n",
        "      \n",
        "        images = Variable(images).cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "\n",
        "        #mixup images and labels\n",
        "        images, labels_a, labels_b, lam = mixup_data(images, labels,\n",
        "                                                       1, 1)\n",
        "        images, labels_a, labels_b = map(Variable, (images,\n",
        "                                                      labels_a, labels_b))\n",
        "        \n",
        "        # Get predictions and confidence value\n",
        "        outputs1, confidence1 =model1(images)\n",
        "        train_total+=outputs1.size(0)\n",
        "        _, predicted = torch.max(outputs1.data, 1)\n",
        "        train_correct+=(lam * predicted.eq(labels_a.data).cpu().sum().float()\n",
        "                    + (1 - lam) * predicted.eq(labels_b.data).cpu().sum().float())\n",
        "\n",
        "        outputs2, confidence2 = model2(images)\n",
        "        train_total2+=outputs2.size(0)\n",
        "        _, predicted = torch.max(outputs2.data, 1)\n",
        "        train_correct2+=(lam * predicted.eq(labels_a.data).cpu().sum().float()\n",
        "                    + (1 - lam) * predicted.eq(labels_b.data).cpu().sum().float())\n",
        "        \n",
        "        # Measure loss value \n",
        "        loss_1, loss_2, pure_ratio_1, pure_ratio_2 = loss_coteaching(outputs1.cpu(), outputs2.cpu(), confidence1.cpu(), confidence2.cpu(), labels_a.cpu(), labels_b.cpu(), rate_schedule[epoch], indexes, noise_or_not, lam, epoch)\n",
        "        \n",
        "        pure_ratio_1_list.append(100*pure_ratio_1)\n",
        "        pure_ratio_2_list.append(100*pure_ratio_2)\n",
        "\n",
        "        # Backward and optimizer\n",
        "        optimizer1.zero_grad()\n",
        "        loss_1.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.zero_grad()\n",
        "        loss_2.backward()\n",
        "        optimizer2.step()\n",
        "        if (i+1) % print_freq == 0:\n",
        "            print ('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f, Pure Ratio1: %.4f, Pure Ratio2 %.4f' \n",
        "                  %(epoch+1, n_epoch, i+1, len(train_set)//batch_size, 100.*train_correct/train_total, 100.*train_correct2/train_total2, loss_1.data, loss_2.data, np.sum(pure_ratio_1_list)/len(pure_ratio_1_list), np.sum(pure_ratio_2_list)/len(pure_ratio_2_list)))\n",
        "\n",
        "    train_acc1=float(train_correct)/float(train_total)\n",
        "    train_acc2=float(train_correct2)/float(train_total2)\n",
        "    return train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list\n",
        "\n",
        "# Evaluate the Model\n",
        "def evaluate(test_loader, model1, model2):\n",
        "    print('Evaluating %s...' % model_str)\n",
        "    model1.eval()    # Change model to 'eval' mode.\n",
        "    correct1 = 0\n",
        "    total1 = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = Variable(images).cuda()\n",
        "        logits1, _ = model1(images)\n",
        "        outputs1 = F.softmax(logits1, dim=1)\n",
        "        _, pred1 = torch.max(outputs1.data, 1)\n",
        "        total1 += labels.size(0)\n",
        "        correct1 += (pred1.cpu() == labels).sum()\n",
        "\n",
        "    model2.eval()    # Change model to 'eval' mode \n",
        "    correct2 = 0\n",
        "    total2 = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = Variable(images).cuda()\n",
        "        logits2, _ = model2(images)\n",
        "        outputs2 = F.softmax(logits2, dim=1)\n",
        "        _, pred2 = torch.max(outputs2.data, 1)\n",
        "        total2 += labels.size(0)\n",
        "        correct2 += (pred2.cpu() == labels).sum()\n",
        " \n",
        "    acc1 = 100*float(correct1)/float(total1)\n",
        "    acc2 = 100*float(correct2)/float(total2)\n",
        "    return acc1, acc2\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('building model...')\n",
        "    cnn1 = resnet34().to(device)\n",
        "    optimizer1 = torch.optim.Adam(cnn1.parameters(), lr=learning_rate)\n",
        "\n",
        "    cnn2 = resnet34().to(device)\n",
        "    optimizer2 = torch.optim.Adam(cnn2.parameters(), lr=learning_rate)\n",
        "\n",
        "    mean_pure_ratio1=0\n",
        "    mean_pure_ratio2=0\n",
        "\n",
        "    with open(txtfile, \"a\") as myfile:\n",
        "        myfile.write('epoch: train_acc1 train_acc2 test_acc1 test_acc2 pure_ratio1 pure_ratio2\\n')\n",
        "\n",
        "\n",
        "    epoch=0\n",
        "    train_acc1=0\n",
        "    train_acc2=0\n",
        "    # evaluate models with random weights\n",
        "    test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
        "    print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %% Pure Ratio1 %.4f %% Pure Ratio2 %.4f %%' % (epoch+1, n_epoch, len(test_set), test_acc1, test_acc2, mean_pure_ratio1, mean_pure_ratio2))\n",
        "    # save results\n",
        "    with open(txtfile, \"a\") as myfile:\n",
        "        myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + ' '  + str(mean_pure_ratio1) + ' '  + str(mean_pure_ratio2) + \"\\n\")\n",
        "\n",
        "    # training\n",
        "    for epoch in range(1, n_epoch):\n",
        "        # train models\n",
        "        cnn1.train()\n",
        "        adjust_learning_rate(optimizer1, epoch)\n",
        "        cnn2.train()\n",
        "        adjust_learning_rate(optimizer2, epoch)\n",
        "        train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list=train(train_loader, epoch, cnn1, optimizer1, cnn2, optimizer2)\n",
        "        # evaluate models\n",
        "        test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
        "        # save results\n",
        "        mean_pure_ratio1 = sum(pure_ratio_1_list)/len(pure_ratio_1_list)\n",
        "        mean_pure_ratio2 = sum(pure_ratio_2_list)/len(pure_ratio_2_list)\n",
        "        print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%, Pure Ratio 1 %.4f %%, Pure Ratio 2 %.4f %%' % (epoch+1, n_epoch, len(test_set), test_acc1, test_acc2, mean_pure_ratio1, mean_pure_ratio2))\n",
        "        with open(txtfile, \"a\") as myfile:\n",
        "            myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + ' ' + str(mean_pure_ratio1) + ' ' + str(mean_pure_ratio2) + \"\\n\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Framework_final.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMerzXSkAGSLTQmQ6CTFX5Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}