{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guzhii/CAS_771_Project/blob/main/Framework_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VXjbvP-yF1BU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Global variables\n",
        "device = \"cuda\"\n",
        "GRAYSCALE = False\n",
        "lr = 0.002\n",
        "result_dir= 'results/'\n",
        "noise_rate = 0\n",
        "forget_rate = None\n",
        "num_gradual = 40\n",
        "exponent = 1\n",
        "top_bn = False\n",
        "dataset = 'cifar10'\n",
        "NUM_CLASSES = 10\n",
        "n_epoch=120\n",
        "seed=1\n",
        "print_freq=50\n",
        "num_workers=4\n",
        "num_iter_per_epoch=500\n",
        "epoch_decay_start=80\n",
        "\n",
        "# Seed\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Hyper Parameters\n",
        "batch_size = 100\n",
        "learning_rate = lr "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZheJS7M1FuyF"
      },
      "outputs": [],
      "source": [
        "####################################################################\n",
        "############### Noisy Loader  ######################################\n",
        "####################################################################\n",
        "\n",
        "def unpickle(file):\n",
        "    import _pickle as cPickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = cPickle.load(fo, encoding='latin1')\n",
        "    return dict\n",
        "\n",
        "\n",
        "class cifar_dataset(Dataset):\n",
        "    def __init__(self, dataset, root_dir, transform, mode, noise_file=''):\n",
        "\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.transition = {0: 0, 2: 0, 4: 7, 7: 7, 1: 1, 9: 1, 3: 5, 5: 3, 6: 6,\n",
        "                           8: 8}  # class transition for asymmetric noise for cifar10\n",
        "        # generate asymmetric noise for cifar100\n",
        "        self.transition_cifar100 = {}\n",
        "        nb_superclasses = 20\n",
        "        nb_subclasses = 5\n",
        "        base = [1, 2, 3, 4, 0]\n",
        "        for i in range(nb_superclasses * nb_subclasses):\n",
        "            self.transition_cifar100[i] = int(base[i % 5] + 5 * int(i / 5))\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            if dataset == 'cifar10':\n",
        "                test_dic = unpickle('%s/test_batch' % root_dir)\n",
        "                self.test_data = test_dic['data']\n",
        "                self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
        "                self.test_data = self.test_data.transpose((0, 2, 3, 1))\n",
        "                self.test_label = test_dic['labels']\n",
        "            elif dataset == 'cifar100':\n",
        "                test_dic = unpickle('%s/test' % root_dir)\n",
        "                self.test_data = test_dic['data']\n",
        "                self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
        "                self.test_data = self.test_data.transpose((0, 2, 3, 1))\n",
        "                self.test_label = test_dic['fine_labels']\n",
        "        else:\n",
        "            train_data = []\n",
        "            train_label = []\n",
        "            if dataset == 'cifar10':\n",
        "                for n in range(1, 6):\n",
        "                    dpath = '%s/data_batch_%d' % (root_dir, n)\n",
        "                    data_dic = unpickle(dpath)\n",
        "                    train_data.append(data_dic['data'])\n",
        "                    train_label = train_label + data_dic['labels']\n",
        "                train_data = np.concatenate(train_data)\n",
        "            elif dataset == 'cifar100':\n",
        "                train_dic = unpickle('%s/train' % root_dir)\n",
        "                train_data = train_dic['data']\n",
        "                train_label = train_dic['fine_labels']\n",
        "                # print(train_label)\n",
        "                # print(len(train_label))\n",
        "            train_data = train_data.reshape((50000, 3, 32, 32))\n",
        "            train_data = train_data.transpose((0, 2, 3, 1))\n",
        "\n",
        "            noise_label = json.load(open(noise_file, \"r\"))\n",
        "\n",
        "            if self.mode == 'train':\n",
        "                self.train_data = train_data\n",
        "                self.noise_label = noise_label\n",
        "                self.clean_label = train_label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.mode == 'train':\n",
        "            img, target = self.train_data[index], self.noise_label[index]\n",
        "            img = Image.fromarray(img)\n",
        "            img = self.transform(img)\n",
        "            return img, target, index\n",
        "        elif self.mode == 'test':\n",
        "            img, target = self.test_data[index], self.test_label[index]\n",
        "            img = Image.fromarray(img)\n",
        "            img = self.transform(img)\n",
        "            return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode != 'test':\n",
        "            return len(self.train_data)\n",
        "        else:\n",
        "            return len(self.test_data)\n",
        "\n",
        "\n",
        "class cifar_dataloader():\n",
        "    def __init__(self, dataset, batch_size, num_workers, root_dir, noise_file=''):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.root_dir = root_dir\n",
        "        self.noise_file = noise_file\n",
        "        if self.dataset == 'cifar10':\n",
        "            self.transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "            ])\n",
        "            self.transform_test = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "            ])\n",
        "        elif self.dataset == 'cifar100':\n",
        "            self.transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n",
        "            ])\n",
        "            self.transform_test = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n",
        "            ])\n",
        "\n",
        "    def run(self, mode):\n",
        "        if mode == 'train':\n",
        "            train_dataset = cifar_dataset(dataset=self.dataset,\n",
        "                                          root_dir=self.root_dir, transform=self.transform_train, mode=\"train\",\n",
        "                                          noise_file=self.noise_file)\n",
        "            trainloader = DataLoader(\n",
        "                dataset=train_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=self.num_workers)\n",
        "            return trainloader, np.asarray(train_dataset.noise_label), np.asarray(train_dataset.clean_label)\n",
        "\n",
        "        elif mode == 'test':\n",
        "            test_dataset = cifar_dataset(dataset=self.dataset,\n",
        "                                         root_dir=self.root_dir, transform=self.transform_test, mode='test')\n",
        "            test_loader = DataLoader(\n",
        "                dataset=test_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=self.num_workers)\n",
        "            return test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SoCdHcZQcEAH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = conv3x3(3, 64) # number 1 indicate how many channels\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "        self.c_linear = nn.Linear(512 * block.expansion,1)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, lin=0, lout=5):\n",
        "        out = x\n",
        "        if lin < 1 and lout > -1:\n",
        "            out = self.conv1(out)\n",
        "            out = self.bn1(out)\n",
        "            out = F.relu(out)\n",
        "        if lin < 2 and lout > 0:\n",
        "            out = self.layer1(out)\n",
        "        if lin < 3 and lout > 1:\n",
        "            out = self.layer2(out)\n",
        "        if lin < 4 and lout > 2:\n",
        "            out = self.layer3(out)\n",
        "        if lin < 5 and lout > 3:\n",
        "            out = self.layer4(out)\n",
        "        if lout > 4:\n",
        "            out = F.avg_pool2d(out, 4)\n",
        "            out = out.view(out.size(0), -1)\n",
        "            feature = out\n",
        "            out_c = self.c_linear(out)\n",
        "            out = self.linear(out)\n",
        "        return out, out_c\n",
        "\n",
        "def resnet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=NUM_CLASSES)\n",
        "\n",
        "\n",
        "# def test():\n",
        "#     net = ResNet34()\n",
        "#     y1, feature1,c1 = net(Variable(torch.randn(3, 3, 32, 32)))\n",
        "#     y2, feature2,c2= net(Variable(torch.randn(3, 3, 32, 32)))\n",
        "#     print(y1.size())\n",
        "#     print(feature1.size())\n",
        "#     print(c1.size())\n",
        "#\n",
        "# test()\n",
        "\n",
        "\n",
        "\n",
        "model = resnet34().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkWmBgOBaP4t",
        "outputId": "07aecda0-fe54-4d51-c561-370f431dce6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "loading dataset...\n"
          ]
        }
      ],
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "import os\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "import argparse, sys\n",
        "import numpy as np\n",
        "import datetime\n",
        "import shutil\n",
        "\n",
        "\n",
        "# load dataset\n",
        "if dataset=='cifar10':\n",
        "    input_channel=3\n",
        "    num_classes=10\n",
        "    top_bn = False\n",
        "    epoch_decay_start = 80\n",
        "    \n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "    data_path = '/content/data/cifar-10-batches-py'  # path to the data file (don't forget to download the feature data and also put the noisy label file under this folder)\n",
        "\n",
        "if dataset=='cifar100':\n",
        "    input_channel=3\n",
        "    num_classes=100\n",
        "    top_bn = False\n",
        "    epoch_decay_start = 100\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "    test_set = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "    data_path = '/content/data/cifar-100-python'  # path to the data file (don't forget to download the feature data and also put the noisy label file under this folder)\n",
        "\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "print('loading dataset...')\n",
        "loader = cifar_dataloader(dataset, batch_size=100,\n",
        "                          num_workers=2,\n",
        "                          root_dir=data_path,\n",
        "                          noise_file='%s/cifar10_noisy_labels_task1.json' % (data_path))\n",
        "\n",
        "train_loader, noisy_labels, clean_labels = loader.run('train')\n",
        "test_loader = loader.run('test')\n",
        "\n",
        "noise_or_not = np.transpose(noisy_labels)==np.transpose(clean_labels)\n",
        "noise_rate = noise_or_not.tolist().count(False) / len(noise_or_not)\n",
        "\n",
        "\n",
        "\n",
        "if forget_rate is None:\n",
        "    forget_rate=noise_rate\n",
        "else:\n",
        "    forget_rate=forget_rate\n",
        "\n",
        "# Adjust learning rate and betas for Adam Optimizer\n",
        "mom1 = 0.9\n",
        "mom2 = 0.1\n",
        "alpha_plan = [learning_rate] * n_epoch\n",
        "beta1_plan = [mom1] * n_epoch\n",
        "for i in range(epoch_decay_start, n_epoch):\n",
        "    alpha_plan[i] = float(n_epoch - i) / (n_epoch - epoch_decay_start) * learning_rate\n",
        "    beta1_plan[i] = mom2\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr']=alpha_plan[epoch]\n",
        "        param_group['betas']=(beta1_plan[epoch], 0.999) # Only change beta1\n",
        "        \n",
        "# define drop rate schedule\n",
        "rate_schedule = np.ones(n_epoch)*forget_rate\n",
        "rate_schedule[:num_gradual] = np.linspace(0, forget_rate**exponent, num_gradual)\n",
        "\n",
        "save_dir = result_dir +'/' +dataset+'/coteaching/'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.system('mkdir -p %s' % save_dir)\n",
        "\n",
        "model_str=dataset+'_coteaching_'+'_'+str(noise_rate)\n",
        "\n",
        "txtfile=save_dir+\"/\"+model_str+str(num_gradual)+\".txt\"\n",
        "nowTime=datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "if os.path.exists(txtfile):\n",
        "    os.system('mv %s %s' % (txtfile, txtfile+\".bak-%s\" % nowTime))\n",
        "\n",
        "\n",
        "def accuracy(logit, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    output = F.softmax(logit, dim=1)\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7uhpD1r8cqUZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "epi = 1e-12\n",
        "ep_threshold = 60\n",
        "momentum = 0.9\n",
        "lossLamda = 0.5\n",
        "regularization_strength = 0.1\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "\n",
        "class CAR(torch.nn.Module):\n",
        "    def __init__(self, labels, num_classes):\n",
        "        super(CAR, self).__init__()\n",
        "        self.num_classes = NUM_CLASSES\n",
        "        self.threshold_update = 0.0\n",
        "\n",
        "        self.soft_labels = torch.zeros(labels.shape[0], num_classes, dtype=torch.float).cuda(non_blocking=True)\n",
        "        self.soft_labels[torch.arange(labels.shape[0]), labels] = 1\n",
        "        self.momentum = momentum\n",
        "        self.beta = regularization_strength\n",
        "        self.es = ep_threshold\n",
        "        self.test = 0\n",
        "        if torch.cuda.is_available():\n",
        "          torch.backends.cudnn.benchmark = True\n",
        "          if torch.cuda.device_count() > 1:\n",
        "              self.device = torch.device('cuda:0')\n",
        "          else:\n",
        "              self.device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "\n",
        "    def forward(self, logits, confidence,  labels, index, lam, epoch):\n",
        "        # sigmoid scale 0 to 1\n",
        "        confidence = torch.sigmoid(confidence)\n",
        "\n",
        "        output = F.softmax(logits, dim=1)\n",
        "        eps = 1e-12\n",
        "        output = torch.clamp(output, 0. + eps, 1. - eps)\n",
        "        confidence = torch.clamp(confidence, 0. + eps, 1. - eps)\n",
        "        one_hot = torch.zeros(len(labels), self.num_classes)\n",
        "        one_hot[torch.arange(len(labels)), labels] = 1\n",
        "        one_hot = one_hot.to(device)\n",
        "        one_hot = torch.clamp(one_hot, min=1e-4, max=1.0)  # A=-4\n",
        "        labels = labels.to(self.device)\n",
        "        confidence = confidence.to(self.device)\n",
        "        output = output.to(self.device)\n",
        "\n",
        "        if epoch < ep_threshold:\n",
        "            pred = confidence * output + (1 - confidence) * one_hot\n",
        "            pred = torch.clamp(pred, min=1e-7, max=1.0)\n",
        "            loss1 = -torch.mean(torch.sum(torch.log(pred) * one_hot, dim=1))\n",
        "            rce = (-1 * torch.sum(pred * torch.log(one_hot), dim=1))\n",
        "        else:\n",
        "            if epoch % 10 == 0:\n",
        "                temp_p = F.softmax(logits.detach(), dim=1)\n",
        "                temp_p = temp_p.to(self.device)\n",
        "                tp_f = confidence > self.threshold_update # only change the data has confidence >= threshold\n",
        "                change_index = index[tp_f.view(tp_f.size()[0])]\n",
        "                tp_f = tp_f.repeat(1, self.num_classes)\n",
        "                self.soft_labels[change_index] = self.momentum * self.soft_labels[change_index] + (\n",
        "                        1 - self.momentum) * temp_p[tp_f].view(-1, self.num_classes)\n",
        "                self.soft_labels = torch.clamp(self.soft_labels, min=1e-4, max=1.0)\n",
        "            pred = confidence * output + (1 - confidence) * self.soft_labels[index]\n",
        "            pred = torch.clamp(pred, min=1e-7, max=1.0)\n",
        "            loss1 = -torch.mean(torch.sum(torch.log(pred) * self.soft_labels[index], dim=1))\n",
        "            rce = (-1 * torch.sum(pred * torch.log(self.soft_labels[index]), dim=1))\n",
        "\n",
        "        loss2 = -torch.mean(torch.log(confidence))\n",
        "        return loss1 + lam * loss2 + self.beta * rce.mean()\n",
        "criterion = CAR(noisy_labels, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yw8uAGp_BEwd"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "\n",
        "def loss_coteaching(y_1, y_2, c_1, c_2, t_a, t_b, forget_rate, ind, noise_or_not, lam, ep_num):\n",
        "    # Calculate and sort cross entropy losses for both models\n",
        "    loss_1_a = F.cross_entropy(y_1, t_a, reduce = False)\n",
        "    loss_1_b = F.cross_entropy(y_1, t_b, reduce = False)\n",
        "    loss_1 = torch.add(torch.mul(loss_1_a, lam), torch.mul(loss_1_b, (1-lam)))\n",
        "    ind_1_sorted = np.argsort(loss_1.cpu().data).cuda()\n",
        "    loss_1_sorted = loss_1[ind_1_sorted]\n",
        "\n",
        "    loss_2_a = F.cross_entropy(y_2, t_a, reduce = False)\n",
        "    loss_2_b = F.cross_entropy(y_2, t_b, reduce = False)\n",
        "    loss_2 = torch.add(torch.mul(loss_2_a, lam), torch.mul(loss_2_b, (1-lam)))\n",
        "    ind_2_sorted = np.argsort(loss_2.cpu().data).cuda()\n",
        "    loss_2_sorted = loss_2[ind_2_sorted]\n",
        "\n",
        "    # Discard high loss samples based on forget rate\n",
        "    remember_rate = 1 - forget_rate\n",
        "    num_remember = int(remember_rate * len(loss_1_sorted))\n",
        "    pure_ratio_1 = np.sum(noise_or_not[ind[ind_1_sorted.cpu()[:num_remember]]])/float(num_remember)\n",
        "    pure_ratio_2 = np.sum(noise_or_not[ind[ind_2_sorted.cpu()[:num_remember]]])/float(num_remember)\n",
        "    ind_1_update=ind_1_sorted[:num_remember]\n",
        "    ind_2_update=ind_2_sorted[:num_remember]\n",
        "\n",
        "    # exchange and update confidence adaptive losses of both models\n",
        "    loss_1_a_update = criterion(y_1[ind_2_update], c_1[ind_2_update], t_a[ind_2_update], ind[ind_2_update], 50, ep_num)\n",
        "    loss_1_b_update = criterion(y_1[ind_2_update], c_1[ind_2_update], t_b[ind_2_update], ind[ind_2_update], 50, ep_num)\n",
        "    # loss_1_a_update = F.cross_entropy(y_1[ind_2_update], t_a[ind_2_update])\n",
        "    # loss_1_b_update = F.cross_entropy(y_1[ind_2_update], t_b[ind_2_update])\n",
        "    loss_1_update = torch.add(torch.mul(loss_1_a_update, lam), torch.mul(loss_1_b_update, (1-lam)))\n",
        "    loss_2_a_update = criterion(y_2[ind_1_update], c_2[ind_1_update], t_a[ind_1_update], ind[ind_1_update], 50, ep_num)\n",
        "    loss_2_b_update = criterion(y_2[ind_1_update], c_2[ind_1_update], t_b[ind_1_update], ind[ind_1_update], 50, ep_num)\n",
        "    # loss_2_a_update = F.cross_entropy(y_2[ind_1_update], t_a[ind_1_update])\n",
        "    # loss_2_b_update = F.cross_entropy(y_2[ind_1_update], t_b[ind_1_update])\n",
        "    loss_2_update = torch.add(torch.mul(loss_2_a_update, lam), torch.mul(loss_2_b_update, (1-lam)))\n",
        "\n",
        "    return loss_1_update, loss_2_update, pure_ratio_1, pure_ratio_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "Kx2vvuCgCd2X",
        "outputId": "4639d43b-a325-4608-9dd2-b155d4ba2cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building model...\n",
            "Evaluating cifar10_coteaching__0.35962...\n",
            "Epoch [1/120] Test Accuracy on the 10000 test images: Model1 9.4700 % Model2 9.2000 % Pure Ratio1 0.0000 % Pure Ratio2 0.0000 %\n",
            "Training cifar10_coteaching__0.35962...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/120], Iter [50/500] Training Accuracy1: 11.2357, Training Accuracy2: 10.2858, Loss1: 3.2189, Loss2: 3.1756, Pure Ratio1: 63.6364, Pure Ratio2 63.5556\n",
            "Epoch [2/120], Iter [100/500] Training Accuracy1: 12.0929, Training Accuracy2: 10.9881, Loss1: 3.1837, Loss2: 3.1991, Pure Ratio1: 64.6566, Pure Ratio2 64.4949\n",
            "Epoch [2/120], Iter [150/500] Training Accuracy1: 12.8031, Training Accuracy2: 11.6570, Loss1: 3.0551, Loss2: 3.1178, Pure Ratio1: 63.8114, Pure Ratio2 63.6835\n",
            "Epoch [2/120], Iter [200/500] Training Accuracy1: 13.3791, Training Accuracy2: 12.2662, Loss1: 3.0284, Loss2: 3.0712, Pure Ratio1: 63.9242, Pure Ratio2 63.8485\n",
            "Epoch [2/120], Iter [250/500] Training Accuracy1: 13.8277, Training Accuracy2: 12.7390, Loss1: 3.1352, Loss2: 3.2399, Pure Ratio1: 63.9071, Pure Ratio2 63.8343\n",
            "Epoch [2/120], Iter [300/500] Training Accuracy1: 14.1940, Training Accuracy2: 13.1551, Loss1: 3.1962, Loss2: 3.1517, Pure Ratio1: 64.0808, Pure Ratio2 64.0034\n",
            "Epoch [2/120], Iter [350/500] Training Accuracy1: 14.5007, Training Accuracy2: 13.5675, Loss1: 3.0186, Loss2: 3.0480, Pure Ratio1: 64.1039, Pure Ratio2 64.0433\n",
            "Epoch [2/120], Iter [400/500] Training Accuracy1: 14.8867, Training Accuracy2: 13.9639, Loss1: 3.0808, Loss2: 3.0631, Pure Ratio1: 64.1869, Pure Ratio2 64.1263\n",
            "Epoch [2/120], Iter [450/500] Training Accuracy1: 15.2697, Training Accuracy2: 14.3465, Loss1: 3.0514, Loss2: 3.1133, Pure Ratio1: 64.2222, Pure Ratio2 64.1504\n",
            "Epoch [2/120], Iter [500/500] Training Accuracy1: 15.6424, Training Accuracy2: 14.6604, Loss1: 3.0238, Loss2: 3.1166, Pure Ratio1: 64.2202, Pure Ratio2 64.1495\n",
            "Evaluating cifar10_coteaching__0.35962...\n",
            "Epoch [2/120] Test Accuracy on the 10000 test images: Model1 30.4600 % Model2 32.2900 %, Pure Ratio 1 64.2202 %, Pure Ratio 2 64.1495 %\n",
            "Training cifar10_coteaching__0.35962...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2d7e16739fb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-2d7e16739fb0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mcnn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mtrain_acc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure_ratio_1_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure_ratio_2_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;31m# evaluate models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mtest_acc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-2d7e16739fb0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch, model1, optimizer1, model2, optimizer2)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtrain_total\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         train_correct+=(lam * predicted.eq(labels_a.data).cpu().sum().float()\n\u001b[0m\u001b[1;32m     31\u001b[0m                     + (1 - lam) * predicted.eq(labels_b.data).cpu().sum().float())\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## Train the Model\n",
        "def train(train_loader,epoch, model1, optimizer1, model2, optimizer2):\n",
        "    print('Training %s...' % model_str)\n",
        "    pure_ratio_list=[]\n",
        "    pure_ratio_1_list=[]\n",
        "    pure_ratio_2_list=[]\n",
        "    \n",
        "    train_total=0\n",
        "    train_correct=0 \n",
        "    train_total2=0\n",
        "    train_correct2=0 \n",
        "\n",
        "    for i, (images, labels, indexes) in enumerate(train_loader):\n",
        "        if i>num_iter_per_epoch:\n",
        "            break\n",
        "      \n",
        "        images = Variable(images).cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "\n",
        "        #mixup images and labels\n",
        "        images, labels_a, labels_b, lam = mixup_data(images, labels,\n",
        "                                                       1, 1)\n",
        "        images, labels_a, labels_b = map(Variable, (images,\n",
        "                                                      labels_a, labels_b))\n",
        "        \n",
        "        # Get predictions and confidence value\n",
        "        outputs1, confidence1 =model1(images)\n",
        "        train_total+=outputs1.size(0)\n",
        "        _, predicted = torch.max(outputs1.data, 1)\n",
        "        train_correct+=(lam * predicted.eq(labels_a.data).cpu().sum().float()\n",
        "                    + (1 - lam) * predicted.eq(labels_b.data).cpu().sum().float())\n",
        "\n",
        "        outputs2, confidence2 = model2(images)\n",
        "        train_total2+=outputs2.size(0)\n",
        "        _, predicted = torch.max(outputs2.data, 1)\n",
        "        train_correct2+=(lam * predicted.eq(labels_a.data).cpu().sum().float()\n",
        "                    + (1 - lam) * predicted.eq(labels_b.data).cpu().sum().float())\n",
        "        \n",
        "        # Measure loss value \n",
        "        loss_1, loss_2, pure_ratio_1, pure_ratio_2 = loss_coteaching(outputs1.cpu(), outputs2.cpu(), confidence1.cpu(), confidence2.cpu(), labels_a.cpu(), labels_b.cpu(), rate_schedule[epoch], indexes, noise_or_not, lam, epoch)\n",
        "        \n",
        "        pure_ratio_1_list.append(100*pure_ratio_1)\n",
        "        pure_ratio_2_list.append(100*pure_ratio_2)\n",
        "\n",
        "        # Backward and optimizer\n",
        "        optimizer1.zero_grad()\n",
        "        loss_1.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.zero_grad()\n",
        "        loss_2.backward()\n",
        "        optimizer2.step()\n",
        "        if (i+1) % print_freq == 0:\n",
        "            print ('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f, Pure Ratio1: %.4f, Pure Ratio2 %.4f' \n",
        "                  %(epoch+1, n_epoch, i+1, len(train_set)//batch_size, 100.*train_correct/train_total, 100.*train_correct2/train_total2, loss_1.data, loss_2.data, np.sum(pure_ratio_1_list)/len(pure_ratio_1_list), np.sum(pure_ratio_2_list)/len(pure_ratio_2_list)))\n",
        "\n",
        "    train_acc1=float(train_correct)/float(train_total)\n",
        "    train_acc2=float(train_correct2)/float(train_total2)\n",
        "    return train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list\n",
        "\n",
        "# Evaluate the Model\n",
        "def evaluate(test_loader, model1, model2):\n",
        "    print('Evaluating %s...' % model_str)\n",
        "    model1.eval()    # Change model to 'eval' mode.\n",
        "    correct1 = 0\n",
        "    total1 = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = Variable(images).cuda()\n",
        "        logits1, _ = model1(images)\n",
        "        outputs1 = F.softmax(logits1, dim=1)\n",
        "        _, pred1 = torch.max(outputs1.data, 1)\n",
        "        total1 += labels.size(0)\n",
        "        correct1 += (pred1.cpu() == labels).sum()\n",
        "\n",
        "    model2.eval()    # Change model to 'eval' mode \n",
        "    correct2 = 0\n",
        "    total2 = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = Variable(images).cuda()\n",
        "        logits2, _ = model2(images)\n",
        "        outputs2 = F.softmax(logits2, dim=1)\n",
        "        _, pred2 = torch.max(outputs2.data, 1)\n",
        "        total2 += labels.size(0)\n",
        "        correct2 += (pred2.cpu() == labels).sum()\n",
        " \n",
        "    acc1 = 100*float(correct1)/float(total1)\n",
        "    acc2 = 100*float(correct2)/float(total2)\n",
        "    return acc1, acc2\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('building model...')\n",
        "    cnn1 = resnet34().to(device)\n",
        "    optimizer1 = torch.optim.Adam(cnn1.parameters(), lr=learning_rate)\n",
        "\n",
        "    cnn2 = resnet34().to(device)\n",
        "    optimizer2 = torch.optim.Adam(cnn2.parameters(), lr=learning_rate)\n",
        "\n",
        "    mean_pure_ratio1=0\n",
        "    mean_pure_ratio2=0\n",
        "\n",
        "    with open(txtfile, \"a\") as myfile:\n",
        "        myfile.write('epoch: train_acc1 train_acc2 test_acc1 test_acc2 pure_ratio1 pure_ratio2\\n')\n",
        "\n",
        "\n",
        "    epoch=0\n",
        "    train_acc1=0\n",
        "    train_acc2=0\n",
        "    # evaluate models with random weights\n",
        "    test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
        "    print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %% Pure Ratio1 %.4f %% Pure Ratio2 %.4f %%' % (epoch+1, n_epoch, len(test_set), test_acc1, test_acc2, mean_pure_ratio1, mean_pure_ratio2))\n",
        "    # save results\n",
        "    with open(txtfile, \"a\") as myfile:\n",
        "        myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + ' '  + str(mean_pure_ratio1) + ' '  + str(mean_pure_ratio2) + \"\\n\")\n",
        "\n",
        "    # training\n",
        "    for epoch in range(1, n_epoch):\n",
        "        # train models\n",
        "        cnn1.train()\n",
        "        adjust_learning_rate(optimizer1, epoch)\n",
        "        cnn2.train()\n",
        "        adjust_learning_rate(optimizer2, epoch)\n",
        "        train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list=train(train_loader, epoch, cnn1, optimizer1, cnn2, optimizer2)\n",
        "        # evaluate models\n",
        "        test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
        "        # save results\n",
        "        mean_pure_ratio1 = sum(pure_ratio_1_list)/len(pure_ratio_1_list)\n",
        "        mean_pure_ratio2 = sum(pure_ratio_2_list)/len(pure_ratio_2_list)\n",
        "        print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%, Pure Ratio 1 %.4f %%, Pure Ratio 2 %.4f %%' % (epoch+1, n_epoch, len(test_set), test_acc1, test_acc2, mean_pure_ratio1, mean_pure_ratio2))\n",
        "        with open(txtfile, \"a\") as myfile:\n",
        "            myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + ' ' + str(mean_pure_ratio1) + ' ' + str(mean_pure_ratio2) + \"\\n\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Framework_final.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMUnX1SIEQZdU3hM+Vw3MyK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}